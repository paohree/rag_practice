# rag_practice
Rag를 실제로 만들어 볼 것임 by langchain.  
일단 해보자.  
local 연결 done!  
git ignore test done!  

대통령 연설문 (이명박 대통령 취임사) 를 바탕으로 오픈ai api로 llm, 임베딩 엔진으로 쓰고 있고 임베딩은 실행 시 작성되는데  
텍스트를 바꾸고 싶으면 chroma_store를 삭제해줘야함! 이미 chroma_store가 있다면 여기서 임베딩 로드를 하고 답변을 만듦.  

벡터는 숫자의 나열임. 청크 하나당 한개의 벡터가 만들어짐.  
텍스트를 입력받으면 내부 신경망 계산을 해서 이 텍스트를 대표하는 고정 길이의 벡터가 1개 생성됨.  
chroma(벡터db)에다가 청크 갯수만큼의 백터가 저장이 되는 것임.  
이렇게 텍스트 sample1.txt를 바탕으로 벡터db가 만들어짐.  

질문이 오면, 질문도 임베딩해서 벡터로 변환해서 질문 벡터와 청크 벡터의 유사도를 계산함  
유사도가 높은 청크를 k개 만큼 반환하는데, 그 청크들을 llm이 답하게 함.  
이 llm 생성 때 비용이 들어감. 처음에 백터db 만들 때랑 llm이 답변 생성할 때 비용이 들어감. 질문이 입력될 때도 비용이 들어감.

총 토큰 수는 문서 길이에 얼추 비례함. 그래서 청크를 잘게 나눈다고 해도 디비 생성할 때 토큰이 더 많이 쓰이지는 않음.  
물론 오버랩 많이 시키면 토큰이 더 많을 수 있음. 
통째로 백터를 만들면 검색이 의미가 없음. 질문이 뭐든 결과는 문서 전체를 줌
근데 띄어쓰기 단위로다가 아주 잘게 청크를 나누면 의미가 너무 약해짐.  
  
자르는 행위를 왜 하냐 하면,  
RAG는 검색해서 필요한 근거를 가져오려고 하는 것임.  
청크 1개로 통치기로 다 넣으면 언제나 그 근거가 통치기가 나옴.
그래서 답변의 퀄리티가 LLM이 마지막 답변 생성할 때의 성능에만 의존하게 된다든 것임.  
  
동작방식) 전체 텍스트를 청크 크기 단위로 잘라서 벡터디비에 저장 -> 사용자에게 질문 받음 -> 질문도 백터화 함 -> 질문의 백터와 제일 비슷한 값을 가지는 청크 가져옴 -> 이 청크 바탕으로 llm이 답변 생성  
  
LLM은 한번에 넣을 수 있는 입력 길이에 제한이 보통 있음.  
pdf 아주 긴거 한번에 통짜로 못 읽는다는 것임.  
성능, 비용, 확장성 때문임.  
  
  
청크 사이즈는 데이터랑 질문이 결정하는데,  
리포트, 법령처럼 문단이 명확한 문서는 문단~짧은 섹션 단위가 유리함. 청크가 중간에서 큰 정도.  
  
짧은 문장, FAQ처럼 조각조각인 문서는 더 청크가 작아도 됨.  
  
근데 대통령 연설문 같은 경우는 문단 흐름이 중요하니까 청크 크게 가져가야 함. 문맥이 필요하면 문맥을 넣어줘야 한다는 것임.  

  
LLM과 RAG은 신경망으로 텍스트를 만든다는 점에서는 비슷함.  
근데 LLM의 원리가 입력을 보고 다음에 올 단어와 토큰 확률을 계산해서 문장을 만든다는 것이면  
RAG은 검색해서 근거를 명확하게 한 다음 이걸 텍스트로 만든다는 것임.  
둘 다 신경망 기반 모델인데, 하나는 그 자체로 생성기고 하나는 생성기에다가 검색기를 붙인 것임!  

오 재밌다

